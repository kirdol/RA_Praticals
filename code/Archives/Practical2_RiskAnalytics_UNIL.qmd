---
title: "Risk Analytics - Practical 2"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

### PART 1: Block maxima approach

#### QUESTION A: Read in the data. Draw an histogram of the daily precipitation values. Which distribution would best fit the data ?

```{r}
# Load and clean data
data_clean_df <- read.csv("/Users/sophie/Desktop/Precipitation_lausanne_full.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
data_clean_df$Date <- as.Date(data_clean_df$Date, format = "%m/%d/%Y")
data_clean_df$Precipitation <- as.numeric(data_clean_df$Precipitation)
data_clean_df <- na.omit(data_clean_df)

# Plot histogram
ggplot(data_clean_df, aes(x = Precipitation)) +
  geom_histogram(binwidth = 5, fill = "pink", color = "black") +
  labs(title = "Histogram of Daily Precipitation Values in Lausanne",
       x = "Daily Precipitation (mm)",
       y = "Frequency") +
  theme_minimal()
```

Based on the histogram, the data is heavily skewed to the right. The majority of the precipitation values are concentrated around zero and a long tail for higher values. There are many small precipitation events and a few extreme ones.

The distribution that will best fit the data is the **gamma distribution** and **Weibull**. Gamma and Weibull distributions are usually used in meteorological processes such as precipitation quantities.

#### QUESTION B: Extract the yearly maximum values. Draw their histogram. Which distribution would best fit the data ?

```{r}
# Extract yearly maximum values
data_clean_df$Year <- format(data_clean_df$Date, "%Y")
yearly_max <- aggregate(Precipitation ~ Year, data = data_clean_df, max)

# Plot histogram
ggplot(yearly_max, aes(x = Precipitation)) +
  geom_histogram(binwidth = 4, fill = "pink", color = "black") +
  labs(title = "Histogram of Yearly Maximum Precipitation Values",
       x = "Yearly Maximum Precipitation (mm)",
       y = "Frequency") +
  theme_minimal()
```

Based on the characteristics of the data (yearly maxima and right-skewed nature), the Generalised Extreme Value (GEV) distribution seems to be the best fit. The GEV distribution is designed to model extreme values like yearly maxima and can accommodate the heavy-tailed nature of the data, where a few large values are observed but are rare.

#### QUESTION C: Fit a linear model to the yearly maximum precipitation values and predict the values for the next 10 years. Provide confidence intervals for your predictions and plot it. Do you think that this a reasonable approach?

```{r}
# Fit linear model
linear_model <- lm(Precipitation ~ as.numeric(Year), data = yearly_max)
summary(linear_model)

# Predict for next 10 years
future_years <- data.frame(Year = as.numeric(2015:2024))
predictions <- predict(linear_model, newdata = future_years, interval = "confidence")

# Plot predictions
ggplot() +
  geom_point(data = yearly_max, aes(x = as.numeric(Year), y = Precipitation, color = "Observed")) +
  geom_line(data = yearly_max, aes(x = as.numeric(Year), y = Precipitation, color = "Observed")) +
  geom_point(data = future_years, aes(x = Year, y = predictions[,1], color = "Predicted")) +
  geom_line(data = future_years, aes(x = Year, y = predictions[,1], color = "Predicted"), linetype = "dashed") +
  geom_ribbon(data = future_years, aes(x = Year, ymin = predictions[,2], ymax = predictions[,3], fill = "95% CI"), alpha = 0.2) +
  labs(title = "Linear Model Predictions for Yearly Maximum Precipitation",
       x = "Year", y = "Yearly Maximum Precipitation (mm)",
       color = "Legend", fill = "Confidence Interval") +
  theme_minimal()
```

The very high p-value (0.9258 \< 0.05) shows that the slope of the regression line is not statistically significant. There is no evidence that the year has a significant effect on yearly maximum precipitation in this dataset. Additionally, the predictions show little change in yearly maximum precipitation over the next 10 years.

**Conclusion:** This linear model does not provide a good fit for the data. There are better models available.

#### QUESTION D: Fit a GEV with constant parameters to the historical yearly max values. We recommend using fevd function in extRemes library or gev.fit function in ismev library. Fit a second GEV model with time varying location parameter. Compare the two models using AIC or BIC. Which one do you recommend using?

```{r}
# Fit GEV models
gev_model_const <- fevd(yearly_max$Precipitation, type = "GEV", method = "MLE")
gev_model_timevar <- fevd(yearly_max$Precipitation, type = "GEV", method = "MLE", loc = ~as.numeric(yearly_max$Year))

# Compare models
summary(gev_model_const)
summary(gev_model_timevar)
```

AIC for the constant model (672.9433 ) is lower compared to the time-varying location model (674.8906). BIC for the constant model is also lower (680.2712 ) than for the time-varying location model (684.6612). Si nce both AIC and BIC values favor the constant parameter model, this suggest that the GEV model with constant parameters is the preferred model for fitting the yearly maximum precipitation data in Lausanne.

#### QUESTION E) Draw diagnostic plots of your GEV fit (for example, using gev.diag function). Is it a good fit?

```{r}
gev_model_const_diag <- gev.fit(yearly_max$Precipitation)
gev.diag(gev_model_const_diag)
```

Probability Plot: Since most points lie close to the line, it suggests that the GEV model is reasonably capturing the general behavior of the precipitation extremes.

Quantile Plot: The majority of the points are also close to the diagonal line, it indicates a good fit.

Return Level Plot: The data points generally follow the fitted line, and most fall within the confidence intervals.

Density Plot: The histogram and density overlay indicate that the fitted GEV distribution aligns with the observed data distribution, although there may be sligh deviations at the tail.

Conclusion: the GEV model with constant parameters provides a reasonably good fit for the yearly maximum precipitation values in Lausanne.

#### QUESTION F) Using the model chosen in the previous parts, predict the 10-year return level. Draw your predictions of the 10-year return levels together with your data.

```{r}
# Calculate 10-year return level
return_period <- 10
return_level <- return.level(gev_model_const, return.period = return_period)

# Add return level to plot
ggplot(yearly_max, aes(x = as.numeric(Year), y = Precipitation)) +
  geom_point(color = "blue") +
  geom_line(color = "blue") +
  geom_hline(yintercept = return_level, color = "red", linetype = "dashed") +
  labs(title = "10-Year Return Level Prediction",
       x = "Year", y = "Precipitation (mm)") +
  annotate("text", x = 2010, y = return_level + 5,
           label = paste("10-Year Return Level:", round(return_level, 2)), color = "red") +
  theme_minimal()

```

The plot displays the yearly maximum precipitation in Lausanne over time, with the 10-year return level of 73.61mm marked by a red dashed line. This return level indicates the precipitation level that is expected to be exceeded once every 10 years.

#### QUESTION G) Broadly speaking, each year, there is a chance of 1/10 that the observed value is above the 10-year return level. Comment on the results for both the linear model prediction (from c) and the GEV approach (from f). How many historical values were above this 10-year return level? Answer the same question with 20, 50 and 85-year return level.

```{r}
# Extract GEV parameters directly from the model
location <- gev_model_const$results$par[1] # Location parameter
scale <- gev_model_const$results$par[2]   # Scale parameter
shape <- gev_model_const$results$par[3]   # Shape parameter
```

```{r}
# Extract GEV parameters from the constant model
location <- gev_model_const$results$par[1]
scale <- gev_model_const$results$par[2]
shape <- gev_model_const$results$par[3]

# Define return periods
return_periods <- c(10, 20, 50, 85)

# Function to calculate return levels for each period
calculate_return_level <- function(return_period) {
  location + (scale / shape) * ((-log(1 - 1 / return_period))^(-shape) - 1)
}

# Calculate return levels
return_levels <- sapply(return_periods, calculate_return_level)

# Display return levels
names(return_levels) <- paste(return_periods, "year return level")
print(return_levels)

# Count how many yearly maximum values exceed each return level
exceedances <- sapply(return_levels, function(level) sum(yearly_max$Precipitation > level))

# Display exceedance counts
exceedances
```

Each year, there is 1/10 chance of exceeding the 10-year return level of 73.61 mm based on the GEV model, with 6 historical exceedances aligning with this probability.

The GEV approach accurately captures the rarity of extreme events, providing 20, 50, and 85-year return levels (82.53 mm, 94.90 mm, and 102.45 mm, respectively), with 4, 2, and 1 exceedances observed for each.

In contrast, the linear model lacks the specificity to estimate extreme events reliably, as it doesn't account for tail behavior. The GEV model's results match expected exceedance frequencies, validating its effectiveness for extreme precipitation analysis.

#### QUESTION H) Using the fitted model, compute the return period of 100 mm of precipitation.

```{r}
# Desired precipitation level
precipitation_level <- 100

# Calculate the return period for 100 mm of precipitation
return_period_100mm <- 1 / (1 - exp(-((precipitation_level - location) / scale * shape + 1)^(-1 / shape)))

cat("The return period for a 100 mm precipitation event is approximately:", return_period_100mm, "years\n")
```

The return period for a 100 mm precipitation event is approximately: 71.70624 years

```{r}
# Desired precipitation threshold
precipitation_threshold <- 150

# Compute the probability of exceeding 150 mm in a given year
prob_exceed_150mm <- 1 - exp(-((precipitation_threshold - location) / scale * shape + 1)^(-1 / shape))

cat("The probability of a precipitation event exceeding 150 mm in the next year is approximately:", prob_exceed_150mm * 100, "%\n")
```

The probability of a precipitation event exceeding 150 mm in the next year is approximately: 0.0643559 %

### PART 2: PEAKS-OVER-THRESHOLD APPROACH

#### QUESTION A) Display a time series plot of the daily precipitation across the data range.

```{r}
# Load necessary libraries
library(ggplot2)

# Ensure that the Date column is in Date format
data_clean_df$Date <- as.Date(data_clean_df$Date, format = "%m/%d/%Y")

# Plot the time series of daily precipitation
ggplot(data_clean_df, aes(x = Date, y = Precipitation)) +
  geom_line(color = "blue") +
  labs(title = "Daily Precipitation in Lausanne (1930-2014)",
       x = "Date",
       y = "Precipitation (mm)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### QUESTION B) We want to model the high precipitation levels using the POT approach. First step is choosing a threshold. Draw Mean Residual Life Plot (for example using mrlplot in POT library) for the full range of your data. Choose a reasonable threshold. In the plot from part a) highlight the data that exceeds this threshold.

```{r}
# Load necessary libraries
library(POT)
library(ggplot2)

# Extract the precipitation values
precipitation_values <- data_clean_df$Precipitation

# Generate the Mean Residual Life Plot
mrlplot(precipitation_values, main = "Mean Residual Life Plot for Daily Precipitation")

# After observing the MRL plot, let's choose a threshold
# Replace the chosen_threshold value with the one you select after reviewing the plot
chosen_threshold <- 50  

# Highlight data exceeding the threshold in the time series plot
data_clean_df$AboveThreshold <- data_clean_df$Precipitation > chosen_threshold

ggplot(data_clean_df, aes(x = Date, y = Precipitation)) +
  geom_line(color = "blue") +
  geom_point(data = subset(data_clean_df, AboveThreshold), aes(x = Date, y = Precipitation), color = "red") +
  labs(title = "Daily Precipitation in Lausanne with Exceedances Highlighted",
       x = "Date",
       y = "Precipitation (mm)") +
  theme_minimal()
```

We chose a threshold of 50 mm, which seems appropriate, as it allows for a stable selection of extreme precipitation events base on the MRL plot.

#### QUESTION C) Fit a GPD for the data exceeding the threshold and draw a diagnostic plot. Is it a reasonable fit? (Hint: if not, you may reconsider the choice of the threshold)

```{r}
install.packages("evd")
library(evd)

# Define the threshold
threshold <- 50

# Fit the GPD model to the exceedances over the threshold
fit_gpd <- fpot(data_clean_df$Precipitation, threshold = threshold)
                  
# Generate diagnostic plots for the GPD fit
plot(fit_gpd)

```

Given the plots, the 50 mm threshold seems to provide an adequate fit without significant deviations. Thus, it is reasonable to proceed with this threshold, as the model sufficiently captures the tail behavior of the date within the confidence intervals. CHANGER ET PRENDRE CELUI DE LODRIKC OU JULIEN

#### QUESTION D) Using the fitted model, compute the 10-year, 20-year, 50-year and 85-year return levels.

```{r}
threshold <- 50 

# Extract parameters from the fitted model
shape <- fit_gpd$estimate["shape"]
scale <- fit_gpd$estimate["scale"]

# Calculate the rate of exceedance (lambda)
n_exceedances <- length(fit_gpd$data)  # Number of exceedances
n_total <- nrow(data_clean_df)  # Total number of data points
lambda <- n_exceedances / n_total

# Define return periods
return_periods <- c(10, 20, 50, 85)

# Function to calculate return level
return_level <- function(T) {
  threshold + (scale / shape) * (((T * lambda)^shape) - 1)
}

# Calculate return levels for each return period
return_levels <- sapply(return_periods, return_level)
names(return_levels) <- paste(return_periods, "year return level")

# Display return levels
return_levels

```

10-year return level: 74.81 mm, 20-year return level: 83.44, 50-year return level: 95.80, 85-year return level: 103.48 CHECKER AVEC LES AUTRES DU GROUPE

#### QUESTION E) Using the fitted model, compute the return period of 100 mm of precipitation.

```{r}
# Display both the shape and scale parameters
print(fit_gpd$estimate)
```

```{r}
# Define the target return level and threshold
target_level <- 100
threshold <- 50  # Replace if a different threshold was used

# Parameters from the fitted GPD model
shape <- 0.09818681
scale <- 9.60205760

# Calculate the exceedance rate (lambda)
n_exceedances <- length(fit_gpd$data)  # Number of exceedances above threshold
n_total <- nrow(data_clean_df)  # Total number of data points
lambda <- n_exceedances / n_total

# Calculate the return period for the 100 mm event
return_period_100mm <- (1 / lambda) * (1 + (shape * (target_level - threshold) / scale))^(1 / shape)

# Display the return period
return_period_100mm
```

```{r}
# 
```

On average, a precipitation event of this magnitude (100 mm) would be expected to occur once every 67 years in Lausanne. The average annual precipitation in Lausanne is around 1150 mm. PAS MEME QUE JEFF COMPARER AVEC GIULIA !!DEMANER AU PROF eux 74

#### QUESTION F) Using the fitted model, compute the probability that there will be a day in the next year when the precipitation exceeds 150 mm.

```{r}
# Define the precipitation threshold (150 mm)
precipitation_threshold <- 150

# Extract the parameters of the fitted GPD model
shape <- fit_gpd$estimate["shape"]
scale <- fit_gpd$estimate["scale"]
threshold <- 50  # The threshold you selected for the POT method

# Calculate the CDF for the 150 mm precipitation level
cdf_150mm <- 1 - (1 + (shape / scale) * (precipitation_threshold - threshold))^(-1 / shape)

# Compute the probability of exceeding 150 mm
prob_exceed_150mm <- 1 - cdf_150mm

cat("The probability of a precipitation event exceeding 150 mm in the next year is approximately:", prob_exceed_150mm * 100, "%\n")

```

The probability of a precipitation event exceeding 150 mm in the next year is approximately: 0.07664709%.

```{r}
# Load necessary package for GEV fitting
library(extRemes)

# Fit the GEV model to the yearly maximum precipitation (or exceedances)
fit_gev <- fevd(yearly_max$Precipitation, type = "GEV", method = "MLE")

# View the model parameters
summary(fit_gev)
# Extract parameters for the GEV model
location <- fit_gev$results$par[1]  # Location parameter
scale <- fit_gev$results$par[2]     # Scale parameter
shape <- fit_gev$results$par[3]     # Shape parameter
# Calculate the probability of exceeding 150 mm using the GEV model
prob_exceed_150_gev <- 1 - pevd(150, loc = location, scale = scale, shape = shape, type = "GEV")

# Display the probability
prob_exceed_150_gev
# Load the necessary package
library(extRemes)

# Fit the GEV model to the yearly maximum precipitation (or exceedances)
fit_gev <- fevd(yearly_max$Precipitation, type = "GEV", method = "MLE")

# Extract parameters for the GEV model
location <- fit_gev$results$par[1]  # Location parameter
scale <- fit_gev$results$par[2]     # Scale parameter
shape <- fit_gev$results$par[3]     # Shape parameter

# Calculate the probability of exceeding 150 mm using the GEV model
prob_exceed_150_gev <- 1 - pevd(150, loc = location, scale = scale, shape = shape, type = "GEV")

# Display the probability
prob_exceed_150_gev

```

```{r}
# Step 1: Compute the probability of exceeding the threshold (150 mm) for a single day
daily_prob_exceed_150mm <- lambda * (1 + (shape * (150 - threshold) / scale))^(-1 / shape)

# Step 2: Calculate the annual probability of at least one day exceeding 150 mm
annual_prob_exceed_150mm <- 1 - (1 - daily_prob_exceed_150mm)^365

# Display the annual probability
annual_prob_exceed_150mm
```

# QUESTION G) Compare the results with the block maxima method. Explain the drawbacks and advantages of using the POT approach compared to the block maxima method. Which method do you prefer?

```{r}
# Load necessary package for GEV fitting
library(extRemes)

# Fit the GEV model to the yearly maximum precipitation (or exceedances)
fit_gev <- fevd(yearly_max$Precipitation, type = "GEV", method = "MLE")

# View the model parameters
summary(fit_gev)
# Extract parameters for the GEV model
location <- fit_gev$results$par[1]  # Location parameter
scale <- fit_gev$results$par[2]     # Scale parameter
shape <- fit_gev$results$par[3]     # Shape parameter
# Calculate the probability of exceeding 150 mm using the GEV model
prob_exceed_150_gev <- 1 - pevd(150, loc = location, scale = scale, shape = shape, type = "GEV")

# Display the probability
prob_exceed_150_gev
# Load the necessary package
library(extRemes)

# Fit the GEV model to the yearly maximum precipitation (or exceedances)
fit_gev <- fevd(yearly_max$Precipitation, type = "GEV", method = "MLE")

# Extract parameters for the GEV model
location <- fit_gev$results$par[1]  # Location parameter
scale <- fit_gev$results$par[2]     # Scale parameter
shape <- fit_gev$results$par[3]     # Shape parameter

# Calculate the probability of exceeding 150 mm using the GEV model
prob_exceed_150_gev <- 1 - pevd(150, loc = location, scale = scale, shape = shape, type = "GEV")

# Display the probability
prob_exceed_150_gev


```

# The block maxima method estimates a 0.064% annual probability of precipitation exceeding 150 mm, while the POT appraoch estimates a slightly higher probability at 0.076%. The block maxima is simpler but loses data by focusing only on yearly maxima, potentially underestimating extremes. In contrast, the POT approach captures all exceedances above a threshold, providing more precise insights into rare events. The POT method is thus preferred for analysing extreme precipitation events.

## PART 3: CLUSTERING AND SEASONAL VARIATIONS

```{r}
# Load Geneva temperature data (assuming the file is now in CSV format)
geneva_data <- read.csv("/Users/Sophie/Desktop/Geneva_temperature.csv")

# Inspect the structure of the data to identify the relevant columns
str(geneva_data)
```

# QUESTION A) Upload the Geneva temperature data. Plot the data. Subset the data for the summer months (June to September).

```{r}
# Create the Date column
geneva_data$Date <- as.Date(paste(geneva_data$Year, geneva_data$Month, geneva_data$Day, sep = "-"))

# Check if Date column was created correctly
str(geneva_data)

# Plot the data using ggplot2
ggplot(geneva_data, aes(x = Date, y = AvgTemperature)) +
  geom_line(color = "pink") +
  labs(title = "Daily Temperature in Geneva",
       x = "Date",
       y = "Temperature (°C)") +
  theme_minimal()

```

```{r}
# Subset the data for summer months (June to September)
summer_data <- subset(geneva_data, Month >= 6 & Month <= 9)

# Check the first few rows of the summer data
head(summer_data)

```

```{r}
# Add a Year column to distinguish each summer period
summer_data$Year <- format(summer_data$Date, "%Y")

# Plot the summer data without connecting lines across years
ggplot(summer_data, aes(x = Date, y = AvgTemperature, group = Year)) +
  geom_line(color = "purple") +
  labs(title = "Geneva Temperature Data (Summer Months)", x = "Date", y = "Average Temperature (°C)") +
  theme_minimal()
```

# QUESTION B) Compute the extremal index of the subsetted series with appropriatelly chosen threshold (for example, you can use extremalindex function in extRemes package). Do the extremes occur in clusters? What is the probability that if the temperature today is extreme (above the chosen threshold) then tomorrow will be also extreme?

```{r}
# Load the extRemes library
if (!require(extRemes)) install.packages("extRemes", dependencies = TRUE)
library(extRemes)

# Determine a threshold for extreme temperatures (e.g., the 95th percentile)
threshold <- quantile(summer_data$AvgTemperature, 0.95, na.rm = TRUE)
cat("Chosen threshold for extreme temperatures:", threshold, "°C\n")
```

```{r}
# Load the extRemes library
if (!require(extRemes)) install.packages("extRemes", dependencies = TRUE)
library(extRemes)

# Determine a threshold for extreme temperatures (e.g., the 95th percentile)
threshold <- quantile(summer_data$AvgTemperature, 0.95, na.rm = TRUE)
cat("Chosen threshold for extreme temperatures:", threshold, "°C\n")

# Identify the days where the temperature exceeds the threshold
extreme_days <- summer_data$AvgTemperature > threshold

# Compute the extremal index
extremal_index <- extremalindex(summer_data$AvgTemperature, threshold = threshold)

# Display the extremal index
cat("The extremal index is:", extremal_index, "\n")

# Calculate the probability that an extreme day is followed by another extreme day
probability_following_extreme <- 1 - extremal_index

# Display the probability
cat("The probability that if today is extreme, tomorrow will also be extreme is:", probability_following_extreme, "\n")

```

# The extremal index of the subsetted series, with a threshold of 24.94°C, is 0.2613 (this value is closer to 0 than 1). Therefore, if the temperature today is extreme, there is a 26.13 % probability that tomorrow’s temperature will also be extreme. While there is some clustering of extreme events, the persistence is not high, suggesting that extreme temperature days are relatively isolated in time.

# QUESTION C) Decluster the data using a suitable threshold. Plot the resulting declustered data. (Hint: you may want to use decluster function in the extRemes package.)

```{r}
# Load the extRemes package if not already loaded
if (!require(extRemes)) install.packages("extRemes", dependencies = TRUE)
library(extRemes)

# Determine a threshold for extreme temperatures (e.g., the 95th percentile)
threshold <- quantile(summer_data$AvgTemperature, 0.95, na.rm = TRUE)
cat("Chosen threshold for extreme temperatures:", threshold, "°C\n")

# Decluster the data using the chosen threshold
declustered_data <- decluster(summer_data$AvgTemperature, threshold = threshold, run.length = 1)

# Update the data frame to include only declustered values (setting non-extreme values to NA for clarity)
summer_data$Declustered <- ifelse(summer_data$AvgTemperature >= threshold, declustered_data, NA)

# Plot the original and declustered data
ggplot() +
  geom_line(data = summer_data, aes(x = Date, y = AvgTemperature), color = "blue", alpha = 0.5) +
  geom_point(data = subset(summer_data, !is.na(Declustered)), aes(x = Date, y = Declustered), color = "red") +
  labs(title = "Declustered Extreme Temperatures in Geneva (Summer Months)", 
       x = "Date", y = "Temperature (°C)") +
  theme_minimal()

```

# The declustered data highlights extreme temperature events (above 24.94°C) as red dots. These events occur sporadically, with occasional clusters, indicating that while extremes can group together, they are generally distinct. The threshold effectively identifies significant temperature spikes without over-clustering.

# QUESTION D)

```{r}
# Load necessary libraries
library(extRemes)

# Step 1: Fit the GPD model to the raw data (data above the chosen threshold)
raw_data <- summer_data$AvgTemperature[summer_data$AvgTemperature >= threshold]  # Extreme values above threshold
fit_raw <- fpot(raw_data, threshold = threshold)  # Fit the GPD to the raw data

# Step 2: Fit the GPD model to the declustered data (data above the chosen threshold)
declustered_data <- summer_data$Declustered[!is.na(summer_data$Declustered)]  # Declustered extreme values
fit_declustered <- fpot(declustered_data, threshold = threshold)  # Fit the GPD to the declustered data

# Step 3: Extract GPD model parameters for both raw and declustered data
shape_raw <- fit_raw$estimate["shape"]  # Shape parameter (xi) for raw data
scale_raw <- fit_raw$estimate["scale"]  # Scale parameter (sigma) for raw data

shape_declustered <- fit_declustered$estimate["shape"]  # Shape parameter (xi) for declustered data
scale_declustered <- fit_declustered$estimate["scale"]  # Scale parameter (sigma) for declustered data

# Step 4: Define a function to calculate the return level using the GPD formula
return_level <- function(shape, scale, threshold, return_period) {
  return(threshold + (scale / shape) * (((return_period * length(raw_data) / length(summer_data$AvgTemperature))^shape) - 1))
}

# Step 5: Calculate the 10-year return level for both raw and declustered data
return_period <- 10  # 10-year return level

# 10-year return level for raw data
return_level_raw <- return_level(shape_raw, scale_raw, threshold, return_period)

# 10-year return level for declustered data
return_level_declustered <- return_level(shape_declustered, scale_declustered, threshold, return_period)

# Step 6: Print the results for both models
cat("10-year return level for raw data:", return_level_raw, "°C\n")
cat("10-year return level for declustered data:", return_level_declustered, "°C\n")



```

# In this analysis, we fitted a Generalized Pareto Distribution (GPD) to both the raw and declustered temperature data for the summer months in Geneva. The 10-year return level for the raw data is 23.69°C, while for the declustered data it is slightly lower at 23.52°C. The declustered model, which accounts for isolated extreme events without clustering, provides a slightly more conservative estimate of extreme temperatures compared to the raw model. 
